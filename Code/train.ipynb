{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1DDE2kMY6z31Kow8SymWZfz9wOSC0_Vx4","authorship_tag":"ABX9TyNintWD6wJipgKAmfzJ3HTy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gVpSN3FPohqp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1595219196159,"user_tz":-330,"elapsed":55030,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"af30a3ea-ae59-4961-d414-243e72505b3b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uf1u6weiOs3h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595319942368,"user_tz":-330,"elapsed":3764,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"7cb93ba0-95f8-4b56-d3f7-21c6a9ff5006"},"source":["import pandas as pd\n","import json\n","import numpy as np\n","import codecs\n","from nltk.tokenize import RegexpTokenizer\n","from keras.layers.embeddings import Embedding\n","from keras.layers import Dense, Input\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","from keras.layers import Bidirectional\n","from keras.preprocessing import sequence\n","from keras.layers import Dropout\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.callbacks import ModelCheckpoint"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"A4yWpbNhSJxN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AU83XczhmtWD","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"cE3ZnJ6zXFJl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595319949021,"user_tz":-330,"elapsed":1307,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}}},"source":["def read_data():\n","    \n","    df_data_sentence = pd.read_table('/content/drive/My Drive/Sentiment Analysis/Data/dictionary.txt')\n","    df_data_sentence_processed = df_data_sentence['Phrase|Index'].str.split('|', expand=True)\n","    df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n","    \n","    df_data_sentiment = pd.read_table('/content/drive/My Drive/Sentiment Analysis/Data/sentiment_labels.txt')\n","    df_data_sentiment_processed = df_data_sentiment['phrase ids|sentiment values'].str.split('|', expand=True)\n","    df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n","\n","    df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how='inner', on='phrase_ids')\n","    return df_processed_all\n","\n","def training_data_split(all_data, spitPercent):\n","\n","    msk = np.random.rand(len(all_data)) < spitPercent\n","    train_only = all_data[msk]\n","    test_and_dev = all_data[~msk]\n","\n","\n","    msk_test = np.random.rand(len(test_and_dev)) <0.5\n","    test_only = test_and_dev[msk_test]\n","    dev_only = test_and_dev[~msk_test]\n","\n","    dev_only.to_csv('dev.csv')\n","    test_only.to_csv('test.csv')\n","    train_only.to_csv('train.csv')\n","\n","    return train_only, test_only, dev_only\n","\n","def extract_constants(df):\n","    maxLen = -1\n","    for phrase in df['Phrase']:\n","        splitLine = phrase.split()\n","        maxLen = max(maxLen, len(splitLine))\n","    return len(df['Phrase']), maxLen\n","\n","\n","def load_embeddings(embedding_path, glove_len):\n","  \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n","  print('loading word embeddings from %s' % embedding_path)\n","  weight_vectors = [np.zeros((glove_len, ))]\n","  word_idx = {}\n","  with codecs.open(embedding_path, encoding='utf-8') as f:\n","    for line in f:\n","      word, vec = line.split(u' ', 1)\n","      word_idx[word.lower()] = len(weight_vectors)\n","      weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n","  word_idx[u'-LRB-'] = word_idx.pop(u'(')\n","  word_idx[u'-RRB-'] = word_idx.pop(u')')\n","  weight_vectors.append(np.random.uniform(\n","      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n","  return np.asarray(np.stack(weight_vectors)), word_idx\n","\n","\n","def get_X(data, word_idx, max_seq_len):\n","\n","\n","    maxSeqLength = max_seq_len\n","    no_rows = len(data)\n","    ids = np.zeros((no_rows, maxSeqLength), dtype='int32')\n","\n","    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n","    idx = 0\n","\n","    for index, row in data.iterrows():\n","\n","\n","        sentence = (row['Phrase'])\n","        #print (sentence)\n","        tokenizer = RegexpTokenizer(r'\\w+')\n","        sentence_words = tokenizer.tokenize(sentence)\n","        i = 0\n","        for word in sentence_words:\n","            word_lwr = word.lower()\n","            try:\n","                ids[idx][i] =  word_idx_lwr[word_lwr]\n","\n","            except Exception as e:\n","                ids[idx][i] = len(word_idx_lwr)+1\n","                # continue\n","            i = i + 1\n","        idx = idx + 1\n","\n","    return ids\n","\n","def get_Y(data):\n","\n","    labels = data['sentiment_values']\n","\n","    lables_float = labels.astype(float)\n","\n","    cats = ['0','1','2','3','4','5','6','7','8','9']\n","    labels_mult = round((lables_float * 10)).astype(int)\n","    dummies = pd.get_dummies(labels_mult, prefix='', prefix_sep='')\n","    dummies = dummies.T.reindex(cats).T.fillna(0)\n","    labels_matrix = dummies.to_numpy()\n","\n","    return labels_matrix\n","\n","def pretrained_embedding_layer(emb_matrix):\n","    print(emb_matrix.shape)\n","    embedding_layer = Embedding(emb_matrix.shape[0],emb_matrix.shape[1], trainable=False)\n","    embedding_layer.build((None,))\n","\n","    embedding_layer.set_weights([emb_matrix])\n","    return embedding_layer\n","\n","def get_example_X(sentence, word_idx, max_seq_len):\n","    maxSeqLength = max_seq_len #Maximum length of sentence\n","    ids = np.zeros((1, maxSeqLength), dtype='int32')\n","    word_idx_lwr =  {k.lower(): v for k, v in word_idx.items()}\n","\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    sentence_words = tokenizer.tokenize(sentence)\n","    i = 0\n","    for word in sentence_words:\n","        word_lwr = word.lower()\n","        try:\n","            ids[0][i] =  word_idx_lwr[word_lwr]\n","\n","        except Exception as e:\n","            ids[0][i] = len(word_idx_lwr)+1\n","            # continue\n","        i = i + 1\n","\n","    return ids\n","def evaluate(sentence, model):\n","    X_example = get_example_X(sentence, word_idx, max_len)\n","    score = model.predict(X_example, batch_size=1, verbose=0)\n","    top_3_index = np.argsort(score)[0][-3:]\n","    top_3_scores = score[0][top_3_index]\n","    top_3_weights = top_3_scores/np.sum(top_3_scores)\n","    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n","    print(single_score_dot)\n","    if(bool(round(single_score_dot))):\n","        print('Positive!')\n","    else:\n","        print('Negative!')\n","def getbool(Y):\n","  ans = []\n","  for y in Y:\n","    top_3_index = np.argsort(y)[-3:]\n","    top_3_scores = y[top_3_index]\n","    top_3_weights = top_3_scores/np.sum(top_3_scores)\n","    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n","    if(bool(round(single_score_dot))):\n","        ans.append(1)\n","    else:\n","        ans.append(0)\n","  return np.asarray(ans)\n","np.random.seed(0)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mYmNbojXFdg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595320025324,"user_tz":-330,"elapsed":72346,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"a02f4bc8-9bcc-4ec1-ebe5-642011b7a038"},"source":["print('Reading data...')\n","df = read_data()\n","df_train = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/Data/train.csv', header = 0)\n","df_test = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/Data/test.csv', header = 0)\n","df_dev = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/Data/dev.csv', header = 0)\n","# print(df_train, df_test, df_dev)\n","m, max_len = extract_constants(df)\n","# print(m, max_len)\n","emb_matrix, word_idx = load_embeddings('/content/drive/My Drive/Sentiment Analysis/Data/glove_6B_100d.txt', 100)\n","X_train = get_X(df_train, word_idx, max_len)\n","X_test = get_X(df_test, word_idx, max_len)\n","X_dev = get_X(df_dev, word_idx, max_len)\n","# print(X_train)\n","# print(X_test)\n","# print(X_dev)\n","Y_train = get_Y(df_train)\n","Y_test = get_Y(df_test)\n","Y_dev = get_Y(df_dev)\n","\n","# print(emb_matrix[400001])\n","# print(df_train, X_train, Y_train)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reading data...\n","loading word embeddings from /content/drive/My Drive/Sentiment Analysis/Data/glove_6B_100d.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YSy-H5YoXFmi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595241491562,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"d4a4f6c3-ca66-4c74-acdd-ad3dbe4f3c9e"},"source":["print(max_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["56\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YmTWuaIyew0q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595320167975,"user_tz":-330,"elapsed":1151,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}}},"source":["path = '/content/drive/My Drive/Sentiment Analysis/Data/'\n","np.save(path+'X_train.npy', X_train)\n","np.save(path+'X_test.npy', X_test)\n","np.save(path+'X_dev.npy', X_dev)\n","np.save(path+'Y_train.npy', Y_train)\n","np.save(path+'Y_test.npy', Y_test)\n","np.save(path+'Y_dev.npy', Y_dev)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTw1sqUYXFqS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1595219314458,"user_tz":-330,"elapsed":9470,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"f42488fe-92b2-41fc-e753-7e3752801ebb"},"source":["emb_layer = pretrained_embedding_layer(emb_matrix)\n","print(emb_matrix, emb_layer.get_weights()[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(400002, 100)\n","[[ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n","   0.27061999]\n"," [-0.10767     0.11053     0.59811997 ... -0.83155     0.45293\n","   0.082577  ]\n"," ...\n"," [-0.10461    -0.50470001 -0.49331    ...  0.42526999 -0.51249999\n","  -0.17054   ]\n"," [ 0.28365001 -0.62629998 -0.44351    ...  0.43678001 -0.82607001\n","  -0.15701   ]\n"," [ 0.00488135  0.02151894  0.01027634 ... -0.04798925  0.032894\n","  -0.04953045]] [[ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [-0.038194   -0.24487     0.72812    ... -0.1459      0.8278\n","   0.27062   ]\n"," [-0.10767     0.11053     0.59812    ... -0.83155     0.45293\n","   0.082577  ]\n"," ...\n"," [-0.10461    -0.5047     -0.49331    ...  0.42527    -0.5125\n","  -0.17054   ]\n"," [ 0.28365    -0.6263     -0.44351    ...  0.43678    -0.82607\n","  -0.15701   ]\n"," [ 0.00488135  0.02151894  0.01027634 ... -0.04798925  0.032894\n","  -0.04953045]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9jeGNqBZpmr2","colab_type":"code","colab":{}},"source":["def make_model(input_shape, emb_matrix):\n","    \n","    phrase_indices = Input(shape=input_shape, dtype = 'int32')\n","    \n","    emb_layer = pretrained_embedding_layer(emb_matrix)\n","    \n","    embeddings = emb_layer(phrase_indices)   \n","    \n","    X = Bidirectional(LSTM(128, return_sequences=True))(embeddings)\n","    # X = Dropout(0.50)(X)\n","    X = Bidirectional(LSTM(128))(X)\n","    X = Dense(1024, activation='relu')(X)\n","    # X = Dropout(0.50)(X)\n","    X = Dense(10, activation='softmax')(X)\n","    \n","    # Create Model instance which converts sentence_indices into X.\n","    model = Model(inputs=phrase_indices, outputs=X)\n","    \n","    ### END CODE HERE ###\n","    # model 8\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csNB1NyjOVYv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"ok","timestamp":1595224774253,"user_tz":-330,"elapsed":1990,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"e25a490e-8559-4e16-995a-5da98feafdba"},"source":["model = make_model((max_len,), emb_matrix)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(400002, 100)\n","Model: \"model_24\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_25 (InputLayer)        (None, 56)                0         \n","_________________________________________________________________\n","embedding_26 (Embedding)     (None, 56, 100)           40000200  \n","_________________________________________________________________\n","bidirectional_46 (Bidirectio (None, 56, 256)           234496    \n","_________________________________________________________________\n","bidirectional_47 (Bidirectio (None, 256)               394240    \n","_________________________________________________________________\n","dense_49 (Dense)             (None, 1024)              263168    \n","_________________________________________________________________\n","dense_50 (Dense)             (None, 10)                10250     \n","=================================================================\n","Total params: 40,902,354\n","Trainable params: 902,154\n","Non-trainable params: 40,000,200\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q2ZqhGgWqj75","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":528},"executionInfo":{"status":"error","timestamp":1595225318972,"user_tz":-330,"elapsed":76434,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"bb3ea49d-7bc9-43b9-d162-e5e3d89561ab"},"source":["checkpoint_path = \"/content/drive/My Drive/Sentiment Analysis/Data/cpnew8.ckpt\"\n","cp_callback = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=False)\n","model.fit(X_train, Y_train, epochs = 9, batch_size = 3000, shuffle=True, callbacks=[cp_callback])\n","model.save_weights(\"/content/drive/My Drive/Sentiment Analysis/Data/model8.h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/9\n","119586/119586 [==============================] - 24s 198us/step - loss: 1.4630 - accuracy: 0.4215\n","\n","Epoch 00001: saving model to /content/drive/My Drive/Sentiment Analysis/Data/cpnew8.ckpt\n","Epoch 2/9\n","119586/119586 [==============================] - 25s 210us/step - loss: 1.4374 - accuracy: 0.4296\n","\n","Epoch 00002: saving model to /content/drive/My Drive/Sentiment Analysis/Data/cpnew8.ckpt\n","Epoch 3/9\n","114000/119586 [===========================>..] - ETA: 1s - loss: 1.4426 - accuracy: 0.4266"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-8632e089ed74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Sentiment Analysis/Data/cpnew8.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcp_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Sentiment Analysis/Data/model8.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"okPlTBr31VS1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595225361524,"user_tz":-330,"elapsed":3665,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"9a6ea515-97b3-4f6b-8d86-c6ac44063ec7"},"source":["weight_path = '/content/drive/My Drive/Sentiment Analysis/Data/cpnew8.ckpt'\n","loaded_model = make_model((max_len,), emb_matrix)\n","loaded_model.load_weights(weight_path)\n","loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# loss_dev, acc_dev = loaded_model.evaluate(X_dev, Y_dev)\n","# print(loss_dev, acc_dev)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(400002, 100)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XjlwWnbqiJnf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595220402879,"user_tz":-330,"elapsed":153599,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"8b75d0ec-596b-4575-a914-236905903835"},"source":["# loss_train, acc_train = loaded_model.evaluate(X_train, Y_train)\n","# print(loss_train, acc_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["119586/119586 [==============================] - 153s 1ms/step\n","1.434077559624677 0.43081966042518616\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BhU9GsdliW71","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595220531080,"user_tz":-330,"elapsed":80957,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"96c0a2dd-0aea-42a9-c356-007c06bdb467"},"source":["# loss_test, acc_test = loaded_model.evaluate(X_test, Y_test)\n","# print(loss_test, acc_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["59985/59985 [==============================] - 80s 1ms/step\n","1.474054392238473 0.41995498538017273\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DzuuE9xrMrE-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1590142026752,"user_tz":-330,"elapsed":1316,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"cea6c728-fd92-4906-817a-5262877bcfec"},"source":["# Low accuracy on classification on 10 classes but high on binary positive/negative classification.\n","# No need for regularization as accuracy on test set is equal to that of the train.\n","# Model can add more LSTM layers to better fit the training set.\n","sentence = 'nice'\n","evaluate(sentence, loaded_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.67\n","Positive!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qRi5qzbriWMI","colab_type":"code","colab":{}},"source":["# p1 = loaded_model.predict(X_train)\n","# np.save('/content/drive/My Drive/Sentiment Analysis/Data/p1.npy', p1)\n","# p2 = loaded_model.predict(X_test)\n","# np.save('/content/drive/My Drive/Sentiment Analysis/Data/p2.npy', p2)\n","p3 = loaded_model.predict(X_dev)\n","np.save('/content/drive/My Drive/Sentiment Analysis/Data/p3.npy', p3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN16D2eFnNDz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595225449686,"user_tz":-330,"elapsed":5420,"user":{"displayName":"Karan Aggarwal","photoUrl":"","userId":"14241836093579771442"}},"outputId":"f502ff3e-7b19-4095-93fd-8c568df77759"},"source":["# p1 = np.load('/content/drive/My Drive/Sentiment Analysis/Data/p1.npy')\n","# p1 = getbool(p1)\n","# Yp1 = getbool(Y_train)\n","# acc_train_bool = np.sum((p1 == Yp1))/p1.shape[0]\n","# p2 = np.load('/content/drive/My Drive/Sentiment Analysis/Data/p2.npy')\n","# p2 = getbool(p2)\n","# Yp2 = getbool(Y_test)\n","# acc_test_bool = np.sum((p2 == Yp2))/p2.shape[0]\n","p3 = np.load('/content/drive/My Drive/Sentiment Analysis/Data/p3.npy')\n","p3 = getbool(p3)\n","Yp3 = getbool(Y_dev)\n","acc_dev_bool = np.sum((p3 == Yp3))/p3.shape[0]\n","# print(acc_train_bool, acc_test_bool, acc_dev_bool)\n","print(acc_dev_bool)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:145: RuntimeWarning: invalid value encountered in true_divide\n"],"name":"stderr"},{"output_type":"stream","text":["0.686338251229474\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"25cYAM3Cn8kC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}